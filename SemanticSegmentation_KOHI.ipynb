{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "SemanticSegmentation_KOHI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttFAeOZo4LE1"
      },
      "source": [
        "# Why do we need segmentation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_yO3FNZj8Xb"
      },
      "source": [
        "# What is Sementic Segmentation?\n",
        "\n",
        "![대체 텍스트](https://miro.medium.com/max/700/1*8Nwk_IdGpe235Nsfewpucg.png)\n",
        "\n",
        "from Fei-Fei Li Stanford Course — Detection And Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrwRsNgKkEOh"
      },
      "source": [
        "# STEP 1: Data loading\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeFjhP_RkIyT"
      },
      "source": [
        "https://www.kaggle.com/nikhilpandey360/lung-segmentation-from-chest-x-ray-dataset\n",
        "\n",
        "![대체 텍스트](https://www.altoros.com/blog/wp-content/uploads/2018/12/segmentation-results-max-dice-score.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6MDbXHSOD4n",
        "outputId": "47d9f41d-4dfc-4d67-bb4d-31dc488db08e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QAmwHjkOGjk",
        "outputId": "21bd09fc-c458-41bc-82ff-54627d219c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO_CD8poMZNa",
        "outputId": "4affc108-1e8a-46dc-c3ea-2d72b2b61e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "!rm -rf *\n",
        "!git clone https://github.com/kevinkwshin/Handson_segmentation.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Handson_segmentation'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9wM4MMSMx8b"
      },
      "source": [
        "%cd Handson_segmentation\n",
        "!cat images.tar.gz* | tar -zxvpf -\n",
        "!cat masks.tar.gz* | tar -zxvpf -\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my6p4ifMf2k2"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLZQly60eEGK"
      },
      "source": [
        "!pip install livelossplot --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMnfrQ3ef2k7"
      },
      "source": [
        "# load library\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# IMG_WIDTH = 512\n",
        "# IMG_HEIGHT = 512\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "data_path = \"./dataset/\"\n",
        "\n",
        "x_list = sorted(glob.glob(data_path+'images/*'))\n",
        "y_list = sorted(glob.glob(data_path+'masks/*'))\n",
        "print(len(x_list),len(y_list),x_list[:4],y_list[:4])\n",
        "\n",
        "X_all = np.zeros((len(x_list), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "y_all = np.zeros((len(y_list), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
        "\n",
        "count = 0\n",
        "for idx in range(len(x_list)):\n",
        "\n",
        "    # img = imread(x_list[idx])\n",
        "    # img = np.expand_dims(img, axis=-1)\n",
        "    img = cv2.imread(x_list[idx])\n",
        "    img = cv2.resize(img,(IMG_WIDTH,IMG_HEIGHT))\n",
        "    \n",
        "    mask = imread(y_list[idx])\n",
        "    mask = cv2.resize(mask,(IMG_WIDTH,IMG_HEIGHT))\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    # print(idx,x_list[idx],img.shape,mask.shape)\n",
        "\n",
        "    X_all[idx] = img\n",
        "    y_all[idx] = mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L32p4-fREdnR"
      },
      "source": [
        "**딥러닝을 위한 데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZmj3GOuEcbU"
      },
      "source": [
        "X_all = X_all.astype('float32') / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyiIJ1P1nZqD"
      },
      "source": [
        "**학습, 검증, 테스트 데이터 셋으로 분리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHyCVv7uoRP2"
      },
      "source": [
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUGW4I6Bf2k-"
      },
      "source": [
        "print('X_train',X_train.shape)\n",
        "print('X_valid',X_valid.shape)\n",
        "print('X_test',X_test.shape)\n",
        "print('y_train',y_train.shape)\n",
        "print('y_valid',y_valid.shape)\n",
        "print('y_test',y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTbANTkypyCe"
      },
      "source": [
        "#STEP 2: Exploratory data analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag8STHc1p7Lp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotTrainData(a,b,c):\n",
        "    for i in range(3):\n",
        "        ix = np.random.randint(0, len(a))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.title(\"X_\" + c)\n",
        "        plt.imshow(a[ix])\n",
        "        plt.axis('off')\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.title(\"y_\" + c)\n",
        "        plt.imshow(np.squeeze(b[ix]), 'gray')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        \n",
        "plotTrainData(X_train,y_train, 'train')\n",
        "plotTrainData(X_valid,y_valid, 'valid')\n",
        "plotTrainData(X_test,y_test, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coeKg3i47EZ6"
      },
      "source": [
        "#STEP 3: VGG16 네트워크 다시보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJy58uzh7PGa"
      },
      "source": [
        "![대체 텍스트](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxbpnkMn73qt"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, UpSampling2D, Add, Conv2DTranspose, BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "def vgg16():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"VGGInput\")\n",
        "    \n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "    \n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "    \n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "    \n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #(8, 8)\n",
        "    x = Conv2D(filters=4096, kernel_size=(8,8), padding='valid', activation=relu)(pool_5)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(4096, activation=relu)(x)\n",
        "    pred = Dense(1000, activation=softmax)(x)\n",
        "        \n",
        "    return Model(inputs=inputs, outputs=pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoqMLDUZ8mtO"
      },
      "source": [
        "**FCN 32 - first fully convolutional network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64jZmCTJ8zFQ"
      },
      "source": [
        "![image interpolation](https://matplotlib.org/_images/interpolation_methods.png)\n",
        "\n",
        "\n",
        "**image interpolation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L22PV2ZP-LeI"
      },
      "source": [
        "#STEP 4: 첫번째 영상분할 모델 (FCN32s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl9qQxDYf2lR"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, UpSampling2D, Add, Activation, Conv2DTranspose, BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "from keras.optimizers import *\n",
        "\n",
        "def fcn32s():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "    \n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "    \n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "    \n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "    \n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #x = Conv2D(filters=4096, kernel_size=(8,8), padding='valid', activation=relu)(pool_5)\n",
        "    #x = Flatten()(x)\n",
        "    #x = Dense(4096, activation=relu)(x)\n",
        "    #pred = Dense(1000, activation=softmax)(x)\n",
        "        \n",
        "    conv_t1 = UpSampling2D(size = (32,32))(pool_5)    \n",
        "    conv_t2 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv_t1)\n",
        "    \n",
        "    pred = Activation('sigmoid')(conv_t2)\n",
        "    \n",
        "    return Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZgXBaTzIw8H"
      },
      "source": [
        "\n",
        "\n",
        "![대체 텍스트](https://jinglescode.github.io/assets/img/posts/unet-03.webp)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHwB52rmFK6P"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    \n",
        "    y_true_f = K.cast(y_true, dtype='float32')\n",
        "\n",
        "    y_true_f = K.flatten(y_true_f)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIyVpusC-XiX"
      },
      "source": [
        "!pip install livelossplot --quiet\n",
        "# visualizer\n",
        "from livelossplot import PlotLossesKeras\n",
        "plotlosses = PlotLossesKeras()\n",
        "\n",
        "# build the model\n",
        "model = fcn32s()\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=['accuracy',dice_coef])\n",
        "print(model.summary())\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1, callbacks=[plotlosses])\n",
        "model.save('fcn-32s.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tZGzrgdCYSM"
      },
      "source": [
        "#STEP 5: 결과 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r72jibHdf2lG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotPredictions(X_train_, y_train_, X_valid_, y_valid_, X_test_, y_test_, simpleFCN):\n",
        "    model = simpleFCN     \n",
        "\n",
        "    ix = np.random.randint(0, len(X_train_))\n",
        "    input_ = X_train_[ix:ix+1]\n",
        "    mask_ = y_train_[ix:ix+1]\n",
        "    preds_train = model.predict(input_)\n",
        "    preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"X_train\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(input_[0])\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Y_train\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(mask_[0][:,:,0], 'gray')\n",
        "    plt.subplot(1,3,3)\n",
        "    ret = model.evaluate(input_, mask_)\n",
        "    plt.title(\"Prediction: %.4f\" % (ret[1]))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(preds_train_t[0][:,:,0], 'gray')\n",
        "    plt.show()\n",
        "    \n",
        "    ix = np.random.randint(0, len(X_valid_))\n",
        "    input_ = X_valid_[ix:ix+1]\n",
        "    mask_ = y_valid_[ix:ix+1]\n",
        "    preds_valid = model.predict(input_)\n",
        "    preds_valid_t = (preds_valid > 0.5).astype(np.uint8)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"X_valid\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(input_[0])\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Y_valid\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(mask_[0][:,:,0], 'gray')\n",
        "    plt.subplot(1,3,3)\n",
        "    ret = model.evaluate(input_, mask_)\n",
        "    plt.title(\"Prediction: %.4f\" % (ret[1]))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(preds_valid_t[0][:,:,0], 'gray')\n",
        "    plt.show()\n",
        "    \n",
        "    ix = np.random.randint(0, len(X_test_))\n",
        "    input_ = X_test_[ix:ix+1]\n",
        "    mask_ = y_test_[ix:ix+1]\n",
        "    preds_test = model.predict(input_)\n",
        "    preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"X_test\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(input_[0])\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Y_test\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(mask_[0][:,:,0], 'gray')\n",
        "    plt.subplot(1,3,3)\n",
        "    ret = model.evaluate(input_, mask_)\n",
        "    plt.title(\"Prediction: %.4f\" % (ret[1]))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(preds_test_t[0][:,:,0], 'gray')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7psbiCpnf2lI"
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXYKgQiqIso8"
      },
      "source": [
        "#STEP 6: 두번째 모델 (FCN8s)\n",
        "**Skip Connection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL2bwqAWIvnD"
      },
      "source": [
        "![대체 텍스트](http://deeplearning.net/tutorial/_images/fcn_schema.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5jurEuEf2lQ"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, UpSampling2D, Add, Conv2DTranspose, BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "def fcn8s():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "    \n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "    \n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "    \n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "    \n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #(8, 8)\n",
        "    #x = Conv2D(filters=4096, kernel_size=(8,8), padding='valid', activation=relu)(pool_5)\n",
        "    #x = Flatten()(x)\n",
        "    #x = Dense(4096, activation=relu)(x)\n",
        "    #pred = Dense(1000, activation=softmax)(x)\n",
        "    \n",
        "    #(8, 8)\n",
        "    conv6 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(pool_5)\n",
        "    conv7 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(conv6)\n",
        "    conv8 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv7)\n",
        "    \n",
        "    #(16, 16)\n",
        "    score_pool4 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_4)\n",
        "    conv_t1 = UpSampling2D(size = (2,2))(conv8)     \n",
        "    fuse_1 = Add()([conv_t1,score_pool4])\n",
        "\n",
        "    #(32, 32)\n",
        "    score_pool3 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_3)\n",
        "    conv_t2 = UpSampling2D(size = (2,2))(fuse_1)\n",
        "    fuse_2 = Add()([conv_t2,score_pool3])\n",
        "   \n",
        "    conv_t3 = UpSampling2D(size = (8,8))(fuse_2)\n",
        "    \n",
        "    pred = Activation('sigmoid')(conv_t3)\n",
        "    \n",
        "    return Model(inputs=inputs, outputs=pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crXsXYp0dHZZ"
      },
      "source": [
        "# visualizer\n",
        "from livelossplot import PlotLossesKeras\n",
        "plotlosses = PlotLossesKeras()\n",
        "\n",
        "# build the model\n",
        "model = fcn8s()\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=['accuracy',dice_coef])\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1, callbacks=[plotlosses])\n",
        "model.save('fcn-8s.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du6uYbZXK8rc"
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ56jKMgaGqq"
      },
      "source": [
        "#STEP 6-1: 두번째 모델 (FCN8s)의 개선시도 -> (FCN2s)\n",
        "**Skip Connection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SwFDrU7SVnO"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, UpSampling2D, Add, Conv2DTranspose, Activation, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import *\n",
        "\n",
        "def fcn2s():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "    \n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "    \n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "    \n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "    \n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #(8, 8)\n",
        "    #x = Conv2D(filters=4096, kernel_size=(8,8), padding='valid', activation=relu)(pool_5)\n",
        "    #x = Flatten()(x)\n",
        "    #x = Dense(4096, activation=relu)(x)\n",
        "    #pred = Dense(1000, activation=softmax)(x)\n",
        "    \n",
        "    #(8, 8)\n",
        "    conv6 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(pool_5)\n",
        "    conv7 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(conv6)\n",
        "    conv8 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv7)\n",
        "\n",
        "    #(16, 16)\n",
        "    score_pool4 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_4)\n",
        "    conv_t1 = UpSampling2D(size = (2,2))(conv8)    \n",
        "    fuse_1 = Add()([conv_t1,score_pool4])\n",
        "\n",
        "    #(32, 32)\n",
        "    score_pool3 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_3)\n",
        "    conv_t2 = UpSampling2D(size = (2,2))(fuse_1)\n",
        "    fuse_2 = Add()([conv_t2,score_pool3])\n",
        "  \n",
        "    #(64, 64)\n",
        "    score_pool2 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_2)\n",
        "    conv_t3 = UpSampling2D(size = (2,2))(fuse_2)\n",
        "    fuse_3 = Add()([conv_t3,score_pool2])\n",
        "\n",
        "    #(128, 128)\n",
        "    score_pool1 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_1)\n",
        "    conv_t4 = UpSampling2D(size = (2,2))(fuse_3)\n",
        "    fuse_4 = Add()([conv_t4,score_pool1])\n",
        "      \n",
        "    conv_t5 = UpSampling2D(size = (2,2))(fuse_4)\n",
        "    \n",
        "    pred = Activation('sigmoid')(conv_t5)\n",
        "    \n",
        "    return Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7idYVQNJ839"
      },
      "source": [
        "# visualizer\n",
        "from livelossplot import PlotLossesKeras\n",
        "plotlosses = PlotLossesKeras()\n",
        "\n",
        "# build the model\n",
        "model = fcn2s()\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=['accuracy',dice_coef])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1, callbacks=[plotlosses])\n",
        "model.save('fcn-2s.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcBFZuJOUyex"
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omi4wo2sM1pw"
      },
      "source": [
        "#STEP 7: 세번째 모델 (FCN8s with deconvolution)\n",
        "**deconvolution**\n",
        "\n",
        "\n",
        "![대체 텍스트](https://miro.medium.com/max/1086/1*AbCrAqPBfkqGRdhKtiZQqA.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DfQeeb4f2lP"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, UpSampling2D, Add, Conv2DTranspose, BatchNormalization, Activation\n",
        "from keras.models import Model\n",
        "from keras.optimizers import *\n",
        "\n",
        "def fcn8s_deconv():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "    \n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "    \n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "    \n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "    \n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #(8, 8)\n",
        "    #x = Conv2D(filters=4096, kernel_size=(8,8), padding='valid', activation=relu)(pool_5)\n",
        "    #x = Flatten()(x)\n",
        "    #x = Dense(4096, activation=relu)(x)\n",
        "    #pred = Dense(1000, activation=softmax)(x)\n",
        "    \n",
        "    #(8, 8)\n",
        "    conv6 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(pool_5)\n",
        "    conv7 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(conv6)\n",
        "    conv8 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv7)\n",
        " \n",
        "    #(16, 16)\n",
        "    score_pool4 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_4)\n",
        "    \n",
        "    conv_t1 = Conv2DTranspose(1, kernel_size=(2,2), strides=(2,2), padding=\"same\")(conv8)\n",
        "    conv_t1 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t1)\n",
        "    conv_t1 = BatchNormalization()(conv_t1)\n",
        "    conv_t1 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t1)\n",
        "    conv_t1 = BatchNormalization()(conv_t1)\n",
        "    \n",
        "    fuse_1 = Add()([conv_t1,score_pool4])\n",
        "\n",
        "    #(32, 32)\n",
        "    score_pool3 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_3)\n",
        "    \n",
        "    conv_t2 = Conv2DTranspose(1, kernel_size=(2,2), strides=(2,2),padding=\"same\")(fuse_1)\n",
        "    conv_t2 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t2)\n",
        "    conv_t2 = BatchNormalization()(conv_t2)\n",
        "    conv_t2 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t2)\n",
        "    conv_t2 = BatchNormalization()(conv_t2)\n",
        "    \n",
        "    fuse_2 = Add()([conv_t2,score_pool3])\n",
        "\n",
        "    #(32, 32) x 8 = 256\n",
        "    conv_t3 = Conv2DTranspose(1, kernel_size=(8,8), strides=(8,8), padding=\"same\")(fuse_2)\n",
        "    conv_t3 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t3)\n",
        "    conv_t3 = BatchNormalization()(conv_t3)\n",
        "    conv_t3 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t3)\n",
        "    conv_t3 = BatchNormalization()(conv_t3)\n",
        "    \n",
        "    pred = Activation('sigmoid')(conv_t3)\n",
        "    \n",
        "    return Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv38OHcsreSC"
      },
      "source": [
        "#model = fcn8s_deconv()\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csKRuJAMNXLQ"
      },
      "source": [
        "# visualizer\n",
        "from livelossplot import PlotLossesKeras\n",
        "plotlosses = PlotLossesKeras()\n",
        "\n",
        "# build the model\n",
        "model = fcn8s_deconv()\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=['accuracy',dice_coef])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1, callbacks=[plotlosses])\n",
        "model.save('fcn-8s_deconv.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfDBHzo_N0Qa"
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2kgJ76NOaG7"
      },
      "source": [
        "#STEP 8: 마지막 모델 (Unet)\n",
        "**concatenation**\n",
        "\n",
        "\n",
        "![대체 텍스트](https://www.renom.jp/notebooks/tutorial/image_processing/u-net/unet.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQOHruaHDNZc"
      },
      "source": [
        "![대체 텍스트](http://2rct3i2488gxf9jvb1lqhek9-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/fcn.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRnxmJKAQOKw"
      },
      "source": [
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras import backend as keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "def unet(input_size=(256,256,3)):\n",
        "    inputs = Input(input_size)\n",
        "    \n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    \n",
        "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    \n",
        "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    \n",
        "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    \n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "\n",
        "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
        "\n",
        "    return Model(inputs=[inputs], outputs=[conv10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puJyRkiDzo--"
      },
      "source": [
        "#model = unet()\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1yIJpPuQseU"
      },
      "source": [
        "# visualizer\n",
        "from livelossplot import PlotLossesKeras\n",
        "plotlosses = PlotLossesKeras()\n",
        "\n",
        "# build the model\n",
        "model = unet()\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=['accuracy',dice_coef])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1, callbacks=[plotlosses])\n",
        "model.save('unet.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zzX76vVSPH8"
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWBsx2wfrq1g"
      },
      "source": [
        "# Take Home Message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hgb_tJwrt-t"
      },
      "source": [
        "* NN-UNET(Official) : https://github.com/MIC-DKFZ/nnUNet\n",
        "* NN-UNET(easy) : https://github.com/kevinkwshin/easy-nnUNet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBS89V3hr2iL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}